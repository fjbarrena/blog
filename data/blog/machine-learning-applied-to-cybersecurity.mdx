---
title: Machine Learning applied to cybersecurity
date: '2020-12-21'
tags: ['ai', 'cybersecurity']
draft: false
summary: 'Discover the Shocking Truth About the Rise of Cybercrime and How AI is Changing the Game!'
---

> This article was originally posted at [ITI Blog](https://www.iti.es/blog/machine-learning-aplicado-a-ciberseguridad/) (in spanish)

# New trends in risk detection based on Machine Learning

For all of us who are dedicated, in one way or another, to cybersecurity, it is clear that we are facing an arms
race between cybercrime and those who defend us from them, the Blue Teams. Not surprisingly, a 2019 study already
shocked us when it found that cybercrime already moves more money than drug trafficking, and the trend in 2020
and 2021 has been on the rise. This can only be explained by the professionalisation of cybercrime, a fact
confirmed by law enforcement authorities.

We can fall into the cliché of thinking of cybercrime as some kind of evil organisation, like Kingpin in Spiderman.
While this cliché is accurate in some cases, there is a large part of cybercrime that is not committed by evil,
criminal organisations, but is perpetrated by companies or individuals who want to make an illegitimate profit, or
want to harm a company or an individual. It is not uncommon to find cybercrime sponsored by a company that wants
to increase the operating costs of its main competitor by creating traffic through bots to force it to scale up
its cloud infrastructure, and thus increase its cost; nor is it uncommon to find employees, or former employees,
exploiting internal company knowledge to undermine its position, or damage it in some way.

Of course, there is also the more familiar version of this story. A criminal organisation impersonates a legitimate
user and executes malicious actions on their behalf. This is made possible by the fragility of users, especially
negligent ones who do not apply company security policies to their accounts, or who use the same password on their
social networks as on their corporate accounts, for example. It is not too complicated to extrapolate password
leaks from cloud services (social networks, video game platform accounts, etc.) to professional accounts, and it
is an increasingly common and dangerous entry vector.

And as you can guess, it's not all malware or ransomware. It's not all mayhem and destruction. There are many
problems that are not as gaudy as ransomware, but can be even more damaging. It takes an average of 280 days for a
company to identify and contain a data breach, according to a recent IBM study. There is therefore plenty of room
for improvement when it comes to detecting and stopping data breaches. Many of these leaks are caused by insiders,
legitimate users of an organisation who conspire against it for personal gain, or to cause harm. Often these
insiders are not conscious users, but rather "puppets" operated by cybercriminals, usually compromised by poor
security policies or user negligence.

In order to detect such attacks earlier, we need to stop looking at what things are, and start looking at how they
behave. From a classic intrusion detection point of view, a user who logs in legitimately from the platform, and
has not had a strange pattern of behaviour from an authentication point of view (e.g. has not had 500 failed
authentications in the last hour...), is legitimate and there is nothing more to talk about. It is not a threat.
But, as we have already advanced before, there are legitimate users who behave maliciously. So we need to look at
HOW a user behaves and not just how they authenticate.

And what technology is particularly good at detecting patterns of behaviour? Artificial intelligence in general,
and machine learning in particular. This is precisely the approach being taken in the OPOSSUM project.

The approach is simple in its conception, but rather more complex in its implementation. Basically, the OPOSSUM
project acts as a reverse proxy between users and applications. In this way, it is in a privileged position to
apply security checks, like a classic Web Application Firewall (WAF), but it is also in a privileged position to
analyse how a user uses a given application, since all communication between the user and the application
necessarily goes through OPOSSUM.

An interesting novelty of this project lies in the concept of context. Classically, cybersecurity solutions have
focused on analysing flat data. Take an HTTP request for example, for a WAF to determine whether the request is
malicious or not, it analyses the content of the request in isolation. This is effective in many cases, but not
for detecting anomalous behaviour. It is true that there are products that take into account a set of data, e.g.
the last 50 requests, but even so, the data on which they are based is still sparse. The OPOSSUM project, on the
other hand, increases the context of the request by enriching the data on which predictions are made, using
external threat intelligence sources such as Shodan, Spyse or Alienvault. These platforms add more information
to the simple HTTP request, such as whether that IP has been involved in security incidents, or whether the
payload of a request contains any indicators of compromise. This enrichment is done in real time, using the
Apache Big Data stack (Hadoop, Kafka, Cassandra, etc.) as well as other interesting technologies as shown in the
figure below.

This enriched data provides much more information, which can be processed by machine learning models to detect
anomalies in user behaviour.

Clearly, new challenges arise, such as updating machine learning models. User behaviour is variable, and evolves
over time. It is also subject to changes in the user's own attributions, such as a natural change of roles due to
a promotion in the organisation, for example. Naturally, in this case, more doors and accesses open for that user,
and their behaviour with corporate applications will vary. Therefore, constant training of these models is a
fundamental requirement. This is why OPOSSUM researches and implements machine learning models based on techniques
such as Adaptive Learning.

The OPOSSUM project is therefore a good example of Machine Learning applied to cybersecurity, which protects
corporate applications from classic attacks, but also from more sophisticated attacks such as data exfiltration,
detection of malicious bots or detection of insiders.
